{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272b589b-87b8-4d1b-a91d-9bb8f54609e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorMemory': '4G', 'executorCores': 4, 'numExecutors': 10, 'conf': {'spark.executorEnv.USERNAME': 'kvaerum', 'spark.executorEnv.HADOOP_DEFAULT_FS': 'hdfs://iccluster067.iccluster.epfl.ch:8020'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4073</td><td>application_1713270977862_4404</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4404/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4404_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>4074</td><td>application_1713270977862_4405</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4405/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4405_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>4084</td><td>application_1713270977862_4413</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4413/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4413_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31298665-7d1a-48a4-bdbd-f88f4c5ca842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4085</td><td>application_1713270977862_4414</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4414/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4414_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Spark name:livy-session-4085, version:3.3.2.3.3.7190.2-1"
     ]
    }
   ],
   "source": [
    "print(f'Start Spark name:{spark._sc.appName}, version:{spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389b902d-c1ad-4373-8396-acad795fdb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local username=kvaerum\n",
      "hadoop_fs=hdfs://iccluster067.iccluster.epfl.ch:8020\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import os\n",
    "default_db = 'com490'\n",
    "username=os.getenv('USER', 'anonymous')\n",
    "hadoop_fs=os.getenv('HADOOP_DEFAULT_FS', 'hdfs://iccluster067.iccluster.epfl.ch:8020')\n",
    "print(f\"local username={username}\\nhadoop_fs={hadoop_fs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d892673-ce68-4907-8d9e-545553636ecd",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d2dc77-4db5-4279-9915-356d15a8e1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://iccluster067.iccluster.epfl.ch:8020/data/geo/json/country=CH"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# List files in the directory\n",
    "files = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration()) \\\n",
    "    .listStatus(sc._jvm.org.apache.hadoop.fs.Path('/data/geo/json'))\n",
    "for file_status in files:\n",
    "    print(file_status.getPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15fbae76-7717-478f-aa6f-043da3cdb021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "geo_df = spark.read.json('/data/geo/json')\n",
    "geo_df = geo_df.filter((geo_df.properties.CANTON == \"VD\") & (geo_df.properties.NAME == \"Lausanne\"))\n",
    "lausanne_geometry = geo_df.select(\"geometry.coordinates\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e9ecb-f013-4f92-abef-04643d0abb83",
   "metadata": {},
   "source": [
    "## Istdaten Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bbeabc-c7da-4c97-865a-94a066673d18",
   "metadata": {},
   "source": [
    "### 1) Select Relevant Columns and Rename for Convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3316c4c-4dee-43ab-8a94-f9917cb5f7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-------+--------------+---------+----------------+-------------------+---------------+----------------+-------------------+---------------+----------------+----------------+\n",
      "|      date|             trip_id|operator_id|stop_id|transportation|cancelled|    arrival_time|  arrival_prognosis|arr_prog_status|  departure_time|departure_prognosis|dep_prog_status|        stop_lat|        stop_lon|\n",
      "+----------+--------------------+-----------+-------+--------------+---------+----------------+-------------------+---------------+----------------+-------------------+---------------+----------------+----------------+\n",
      "|26.09.2023|      85:11:8958:001|      85:11|8502128|           Zug|    false|26.09.2023 15:44|26.09.2023 15:45:41|           REAL|26.09.2023 15:44|26.09.2023 15:46:28|           REAL|47.4286409197269|8.16662018167874|\n",
      "|17.04.2023|      85:11:8925:001|      85:11|8502128|           Zug|    false|17.04.2023 07:08|17.04.2023 07:08:46|           REAL|17.04.2023 07:08|17.04.2023 07:09:15|           REAL|47.4286409197269|8.16662018167874|\n",
      "|23.01.2023|        85:834:54423|     85:834|8588351|           Bus|    false|23.01.2023 14:50|23.01.2023 14:50:34|           REAL|23.01.2023 14:50|23.01.2023 14:51:00|           REAL|46.8064450193664|7.15104780338173|\n",
      "|07.06.2023|85:823:63360-00018-1|     85:823|8592442|          Tram|    false|07.06.2023 16:49|07.06.2023 16:50:51|           REAL|07.06.2023 16:49|07.06.2023 16:51:03|           REAL|47.5571314165509|7.56497352005594|\n",
      "|30.08.2023|     85:82:11494:002|      85:82|8506322|           Zug|    false|31.08.2023 00:04|31.08.2023 00:03:44|           REAL|31.08.2023 00:04|31.08.2023 00:04:14|           REAL|47.4755904408742|9.48936248827034|\n",
      "+----------+--------------------+-----------+-------+--------------+---------+----------------+-------------------+---------------+----------------+-------------------+---------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- arrival_prognosis: string (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- departure_prognosis: string (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      "\n",
      "763642603"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "istdaten_df = spark.read.orc('/data/sbb/orc/istdaten/year=2023')\n",
    "\n",
    "istdaten_df = istdaten_df.select([\n",
    "      istdaten_df['betriebstag'].alias('date'),\n",
    "      istdaten_df['fahrt_bezeichner'].alias('trip_id'),\n",
    "      istdaten_df['betreiber_id'].alias('operator_id'),\n",
    "      istdaten_df['bpuic'].alias('stop_id'),\n",
    "      istdaten_df['produkt_id'].alias('transportation'),\n",
    "      istdaten_df['faellt_aus_tf'].alias('cancelled'),\n",
    "      istdaten_df['ankunftszeit'].alias('arrival_time'),\n",
    "      istdaten_df['an_prognose'].alias('arrival_prognosis'),\n",
    "      istdaten_df['an_prognose_status'].alias('arr_prog_status'),\n",
    "      istdaten_df['abfahrtszeit'].alias('departure_time'),\n",
    "      istdaten_df['ab_prognose'].alias('departure_prognosis'),\n",
    "      istdaten_df['ab_prognose_status'].alias('dep_prog_status'),\n",
    "])\n",
    "\n",
    "stops_df = spark.read.csv('/data/sbb/csv/timetables/stops', header=True)\\\n",
    "                .select(['stop_id','stop_lat', 'stop_lon'])\\\n",
    "                .drop(*['year','month','day'])\\\n",
    "                .dropDuplicates(['stop_id'])\\\n",
    "                .withColumnRenamed(\"stop_id\", \"stops_stop_id\")\\\n",
    "                .withColumn(\"stop_lat\", col(\"stop_lat\").cast(DoubleType()))\\\n",
    "                .withColumn(\"stop_lon\", col(\"stop_lon\").cast(DoubleType()))\n",
    "\n",
    "istdaten_df = istdaten_df.join(\n",
    "    stops_df,\n",
    "    istdaten_df.stop_id == stops_df.stops_stop_id,\n",
    "    \"left\"\n",
    ").drop('stops_stop_id')\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1605f05c-a970-4b33-9b15-175acd6f597a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "\n",
      "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 15, in is_within_lausanne\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/matplotlib/path.py\", line 129, in __init__\n",
      "    vertices = _to_unmasked_float_array(vertices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py\", line 1345, in _to_unmasked_float_array\n",
      "    return np.asarray(x, float)\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3130,) + inhomogeneous part.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 606, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 196, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.PythonException: \n",
      "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 15, in is_within_lausanne\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/matplotlib/path.py\", line 129, in __init__\n",
      "    vertices = _to_unmasked_float_array(vertices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py\", line 1345, in _to_unmasked_float_array\n",
      "    return np.asarray(x, float)\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3130,) + inhomogeneous part.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Parse the strings and remove the altitude\n",
    "lausanne_polygon = []\n",
    "for coord_str in lausanne_geometry[0][0]:\n",
    "    coord = json.loads(coord_str.replace('\\'', '\\\"'))\n",
    "    lausanne_polygon.append((coord[0], coord[1]))\n",
    "\n",
    "# Broadcast the polygon to all nodes\n",
    "broadcast_polygon = spark.sparkContext.broadcast(lausanne_polygon)\n",
    "\n",
    "# Define a UDF to check if a point is within the Lausanne polygon\n",
    "def is_within_lausanne(lat, lon):\n",
    "    point = (lon, lat)  # Note: Point takes (lon, lat)\n",
    "    polygon_path = Path(broadcast_polygon.value)\n",
    "    return polygon_path.contains_point(point)\n",
    "\n",
    "# Register the UDF\n",
    "is_within_lausanne_udf = udf(is_within_lausanne, BooleanType())\n",
    "\n",
    "istdaten_df = istdaten_df.filter(is_within_lausanne_udf(istdaten_df.stop_lat, istdaten_df.stop_lon))\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1792fd5a-f834-4deb-893f-4f3eecac4eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.broadcast.Broadcast object at 0x7fc19c413af0>"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of lausanne_geometry\n",
    "print(broadcast_polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efc8cc-4646-4377-823b-42680e4eb955",
   "metadata": {},
   "source": [
    "### 2) Filtering and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9234fae2-fcce-4a51-b0d6-5823c2a3ceea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GESCHAETZT', 'PROGNOSE', '', 'REAL', 'UNBEKANNT']"
     ]
    }
   ],
   "source": [
    "distinct_arr_prog_status = istdaten_df.select(\"arr_prog_status\").distinct().collect()\n",
    "distinct_arr_prog_status_list = [row.arr_prog_status for row in distinct_arr_prog_status]\n",
    "print(distinct_arr_prog_status_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c86eeb2-6cce-42f2-a363-b998fb1de332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+\n",
      "|               date|       trip_id|operator_id|stop_id|transportation|cancelled|       arrival_time|  arrival_prognosis|arr_prog_status|     departure_time|departure_prognosis|dep_prog_status|\n",
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506101|           Zug|    false|2023-12-22 09:36:00|2023-12-22 09:36:34|           REAL|2023-12-22 09:36:00|2023-12-22 09:36:53|           REAL|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506102|           Zug|    false|2023-12-22 09:39:00|2023-12-22 09:39:08|           REAL|2023-12-22 09:39:00|2023-12-22 09:39:26|           REAL|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506103|           Zug|    false|2023-12-22 09:42:00|2023-12-22 09:42:27|           REAL|2023-12-22 09:42:00|2023-12-22 09:43:48|           REAL|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506104|           Zug|    false|2023-12-22 09:45:00|2023-12-22 09:46:15|           REAL|2023-12-22 09:45:00|2023-12-22 09:46:40|           REAL|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506105|           Zug|    false|2023-12-22 09:50:00|2023-12-22 09:50:40|           REAL|               null|               null|               |\n",
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- arrival_prognosis: timestamp (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- departure_prognosis: timestamp (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      "\n",
      "705684079"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "format_date = \"dd.MM.yyyy\"\n",
    "format_timetable = \"dd.MM.yyyy HH:mm\"\n",
    "format_prognosis = \"dd.MM.yyyy HH:mm:ss\"\n",
    "\n",
    "istdaten_df = istdaten_df.withColumn(\"date\", to_timestamp(col(\"date\"), format_date))\\\n",
    "                         .withColumn(\"arrival_time\", to_timestamp(col(\"arrival_time\"), format_timetable))\\\n",
    "                         .withColumn(\"arrival_prognosis\", to_timestamp(col(\"arrival_prognosis\"), format_prognosis))\\\n",
    "                         .withColumn(\"departure_time\", to_timestamp(col(\"departure_time\"), format_timetable))\\\n",
    "                         .withColumn(\"departure_prognosis\", to_timestamp(col(\"departure_prognosis\"), format_prognosis))\n",
    "\n",
    "istdaten_df = istdaten_df.filter((col(\"arr_prog_status\") != \"\") & (col(\"arr_prog_status\") != \"UNBEKANNT\") & (col('cancelled') == 'false'))\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e5656-ba7e-433c-9994-8eec3b7f6db1",
   "metadata": {},
   "source": [
    "### 3) Istdaten Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fcb7ba9-920e-4a00-b384-6d28979230bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "|               date|       trip_id|operator_id|stop_id|transportation|cancelled|       arrival_time|  arrival_prognosis|arr_prog_status|     departure_time|departure_prognosis|dep_prog_status|delay_arrival|delay_departure|delay_at_station|year|month|day|day_of_week|is_weekday|is_weekend|is_peak_time|\n",
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506101|           Zug|    false|2023-12-22 09:36:00|2023-12-22 09:36:34|           REAL|2023-12-22 09:36:00|2023-12-22 09:36:53|           REAL|           34|             53|              19|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506102|           Zug|    false|2023-12-22 09:39:00|2023-12-22 09:39:08|           REAL|2023-12-22 09:39:00|2023-12-22 09:39:26|           REAL|            8|             26|              18|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506103|           Zug|    false|2023-12-22 09:42:00|2023-12-22 09:42:27|           REAL|2023-12-22 09:42:00|2023-12-22 09:43:48|           REAL|           27|            108|              81|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506104|           Zug|    false|2023-12-22 09:45:00|2023-12-22 09:46:15|           REAL|2023-12-22 09:45:00|2023-12-22 09:46:40|           REAL|           75|            100|              25|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506105|           Zug|    false|2023-12-22 09:50:00|2023-12-22 09:50:40|           REAL|               null|               null|               |           40|           null|            null|2023|   12| 22|          4|      true|     false|       false|\n",
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- arrival_prognosis: timestamp (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- departure_prognosis: timestamp (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      " |-- delay_arrival: long (nullable = true)\n",
      " |-- delay_departure: long (nullable = true)\n",
      " |-- delay_at_station: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_peak_time: boolean (nullable = false)\n",
      "\n",
      "705684079"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, month, dayofmonth, dayofweek, expr, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "istdaten_df = istdaten_df.withColumn(\"delay_arrival\", (col(\"arrival_prognosis\").cast(\"long\") - col(\"arrival_time\").cast(\"long\")))\\\n",
    "                         .withColumn(\"delay_departure\", (col(\"departure_prognosis\").cast(\"long\") - col(\"departure_time\").cast(\"long\")))\\\n",
    "                         .withColumn(\"delay_at_station\", (col(\"delay_departure\") - col(\"delay_arrival\")))\\\n",
    "                         .withColumn(\"year\", year(col(\"date\")))\\\n",
    "                         .withColumn(\"month\", month(col(\"date\")))\\\n",
    "                         .withColumn(\"day\", dayofmonth(col(\"date\")))\\\n",
    "                         .withColumn(\"day_of_week\", (dayofweek(col(\"date\")) + 5) % 7)\\\n",
    "                         .withColumn(\"is_weekday\", (col(\"day_of_week\") <= 4))\\\n",
    "                         .withColumn(\"is_weekend\", (col(\"day_of_week\") > 4))\\\n",
    "                         .withColumn(\"is_peak_time\",\n",
    "                                     when((col(\"is_weekday\")) & \n",
    "                                          ((col(\"arrival_time\").between(expr(\"make_timestamp(year(date), month(date), day(date), 6, 30, 0)\"), \n",
    "                                                                        expr(\"make_timestamp(year(date), month(date), day(date), 8, 30, 0)\"))) |\n",
    "                                           (col(\"arrival_time\").between(expr(\"make_timestamp(year(date), month(date), day(date), 16, 30, 0)\"), \n",
    "                                                                        expr(\"make_timestamp(year(date), month(date), day(date), 18, 30, 0)\")))), True)\n",
    "                                     .otherwise(False))\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd210b-4c5b-493d-8634-fb208efe803f",
   "metadata": {},
   "source": [
    "### 4) Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e7e6e0-bc66-44a6-91b3-66f9ad8e84f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+\n",
      "|               date|             trip_id|operator_id|stop_id|transportation|cancelled|       arrival_time|  arrival_prognosis|arr_prog_status|     departure_time|departure_prognosis|dep_prog_status|delay_arrival|delay_departure|delay_at_station|year|month|day|day_of_week|is_weekday|is_weekend|is_peak_time|        stop_lat|        stop_lon|\n",
      "+-------------------+--------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+\n",
      "|2023-03-18 00:00:00|85:766:202055-621...|     85:766|8570850|           Bus|    false|2023-03-18 20:40:00|2023-03-18 20:43:30|           REAL|               null|               null|               |          210|           null|            null|2023|    3| 18|          5|     false|      true|       false|46.8756220371815|9.52330982286194|\n",
      "|2023-07-04 00:00:00|85:773:67276-14787-1|     85:773|8580432|           Bus|    false|2023-07-04 16:36:00|2023-07-04 16:37:23|           REAL|2023-07-04 16:36:00|2023-07-04 16:38:16|           REAL|           83|            136|              53|2023|    7|  4|          1|      true|     false|        true|47.4439106426415|8.57842587428213|\n",
      "|2023-11-06 00:00:00|85:151:TL031-4506...|     85:151|8588156|           Bus|    false|2023-11-06 18:39:00|2023-11-06 18:40:29|       PROGNOSE|2023-11-06 18:39:00|2023-11-06 18:40:45|       PROGNOSE|           89|            105|              16|2023|   11|  6|          0|      true|     false|       false|46.5112787626392|6.55061386739266|\n",
      "|2023-11-06 00:00:00|85:151:TL031-4506...|     85:151|8588156|           Bus|    false|2023-11-06 19:49:00|2023-11-06 19:49:00|       PROGNOSE|2023-11-06 19:49:00|2023-11-06 19:49:00|       PROGNOSE|            0|              0|               0|2023|   11|  6|          0|      true|     false|       false|46.5112787626392|6.55061386739266|\n",
      "|2023-01-09 00:00:00|       85:889:1125-1|     85:889|8593358|           Bus|    false|2023-01-09 08:56:00|2023-01-09 08:56:29|           REAL|2023-01-09 08:56:00|2023-01-09 08:56:52|           REAL|           29|             52|              23|2023|    1|  9|          0|      true|     false|       false|47.1430328284931|7.27217663630926|\n",
      "+-------------------+--------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- arrival_prognosis: timestamp (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- departure_prognosis: timestamp (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      " |-- delay_arrival: long (nullable = true)\n",
      " |-- delay_departure: long (nullable = true)\n",
      " |-- delay_at_station: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_peak_time: boolean (nullable = false)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      "\n",
      "705684079"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "stops_df = spark.read.csv('/data/sbb/csv/timetables/stops', header=True)\\\n",
    "                .select(['stop_id','stop_lat', 'stop_lon'])\\\n",
    "                .drop(*['year','month','day'])\\\n",
    "                .dropDuplicates(['stop_id'])\\\n",
    "                .withColumnRenamed(\"stop_id\", \"stops_stop_id\")\\\n",
    "                .withColumn(\"stop_lat\", col(\"stop_lat\").cast(DoubleType()))\\\n",
    "                .withColumn(\"stop_lon\", col(\"stop_lon\").cast(DoubleType()))\n",
    "\n",
    "istdaten_df = istdaten_df.join(\n",
    "    stops_df,\n",
    "    istdaten_df.stop_id == stops_df.stops_stop_id,\n",
    "    \"left\"\n",
    ").drop('stops_stop_id')\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58941c2-5b49-42f8-b4ac-7bd989f60da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, count, lag, col, to_timestamp, concat_ws, lit, lpad\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "# Haversine formula to calculate the distance between two points on the earth\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    if lat1 is None or lon1 is None or lat2 is None or lon2 is None:\n",
    "        return None\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c  # Distance in kilometers\n",
    "    return distance\n",
    "\n",
    "# Register the UDF\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd21b49b-09d5-41de-96a1-dda1474a7d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+--------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+-------------+--------------+------------+-------------------+-------------+-------------+------------------+---------------------+\n",
      "|               date|            trip_id|operator_id|stop_id|transportation|cancelled|       arrival_time|  arrival_prognosis|arr_prog_status|departure_time|departure_prognosis|dep_prog_status|delay_arrival|delay_departure|delay_at_station|year|month|day|day_of_week|is_weekday|is_weekend|is_peak_time|        stop_lat|        stop_lon|trip_stop_seq|trip_num_stops|prev_stop_id|prev_departure_time|prev_stop_lat|prev_stop_lon|traveling_time_min|traveling_distance_km|\n",
      "+-------------------+-------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+--------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+-------------+--------------+------------+-------------------+-------------+-------------+------------------+---------------------+\n",
      "|2023-01-05 00:00:00|80:06____:17019:000|  80:06____|8500090|           Zug|    false|2023-01-05 17:23:00|2023-01-05 17:32:00|       PROGNOSE|          null|               null|               |          540|           null|            null|2023|    1|  5|          3|      true|     false|        true|47.5673090682292|7.60691586067732|            1|             1|        null|               null|         null|         null|              null|                 null|\n",
      "|2023-01-07 00:00:00|80:06____:17171:000|  80:06____|8500090|           Zug|    false|2023-01-07 13:50:00|2023-01-07 13:54:00|       PROGNOSE|          null|               null|               |          240|           null|            null|2023|    1|  7|          5|     false|      true|       false|47.5673090682292|7.60691586067732|            1|             1|        null|               null|         null|         null|              null|                 null|\n",
      "|2023-01-10 00:00:00|80:06____:17256:000|  80:06____|8500090|           Zug|    false|2023-01-10 13:08:00|2023-01-10 13:08:00|       PROGNOSE|          null|               null|               |            0|           null|            null|2023|    1| 10|          1|      true|     false|       false|47.5673090682292|7.60691586067732|            1|             1|        null|               null|         null|         null|              null|                 null|\n",
      "|2023-01-12 00:00:00|80:06____:17276:000|  80:06____|8500090|           Zug|    false|2023-01-12 17:38:00|2023-01-12 17:38:00|       PROGNOSE|          null|               null|               |            0|           null|            null|2023|    1| 12|          3|      true|     false|        true|47.5673090682292|7.60691586067732|            1|             1|        null|               null|         null|         null|              null|                 null|\n",
      "|2023-10-11 00:00:00|80:800631:17234:000|  80:800631|8500090|           Zug|    false|2023-10-11 05:56:00|2023-10-11 05:58:00|       PROGNOSE|          null|               null|               |          120|           null|            null|2023|   10| 11|          2|      true|     false|       false|47.5673090682292|7.60691586067732|            1|             1|        null|               null|         null|         null|              null|                 null|\n",
      "+-------------------+-------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+--------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+-------------+--------------+------------+-------------------+-------------+-------------+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- arrival_prognosis: timestamp (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- departure_prognosis: timestamp (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      " |-- delay_arrival: long (nullable = true)\n",
      " |-- delay_departure: long (nullable = true)\n",
      " |-- delay_at_station: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_peak_time: boolean (nullable = false)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      " |-- trip_stop_seq: integer (nullable = false)\n",
      " |-- trip_num_stops: long (nullable = false)\n",
      " |-- prev_stop_id: string (nullable = true)\n",
      " |-- prev_departure_time: timestamp (nullable = true)\n",
      " |-- prev_stop_lat: double (nullable = true)\n",
      " |-- prev_stop_lon: double (nullable = true)\n",
      " |-- traveling_time_min: double (nullable = true)\n",
      " |-- traveling_distance_km: double (nullable = true)\n",
      "\n",
      "705684079"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, count\n",
    "\n",
    "# Define the window spec for calculating trip_stop_seq with year, month, and day\n",
    "trip_stop_seq_window = Window.partitionBy(\"trip_id\", \"year\", \"month\", \"day\").orderBy(\"arrival_time\")\n",
    "istdaten_df = istdaten_df.withColumn(\"trip_stop_seq\", row_number().over(trip_stop_seq_window))\n",
    "\n",
    "# Define the window spec for calculating trip_num_stops with year, month, and day\n",
    "trip_num_stops_window = Window.partitionBy(\"trip_id\", \"year\", \"month\", \"day\")\n",
    "istdaten_df = istdaten_df.withColumn(\"trip_num_stops\", count(\"stop_id\").over(trip_num_stops_window))\n",
    "\n",
    "# Add columns for the previous stop_id and departure_time\n",
    "window_spec_seq = Window.partitionBy(\"trip_id\", \"year\", \"month\", \"day\").orderBy(\"trip_stop_seq\")\n",
    "istdaten_df = istdaten_df.withColumn(\"prev_stop_id\", lag(\"stop_id\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"prev_departure_time\", lag(\"departure_time\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"prev_stop_lat\", lag(\"stop_lat\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"prev_stop_lon\", lag(\"stop_lon\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"traveling_time_min\", ((col(\"arrival_time\").cast(\"long\") - col(\"prev_departure_time\").cast(\"long\"))) / 60)\\\n",
    "                         .withColumn(\"traveling_distance_km\", haversine_udf(col(\"prev_stop_lat\"), col(\"prev_stop_lon\"),col(\"stop_lat\"), col(\"stop_lon\")))\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446fbbb-392a-4d8b-aa04-029302ae8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Calculate the distance between each stop and each weather station\n",
    "wstations_df = spark.read.csv('/data/wunderground/csv/stations', header=True)\n",
    "distance_df = stops_df.crossJoin(wstations_df).withColumn(\n",
    "    \"distance\",\n",
    "    haversine_udf(col(\"stop_lat\"),col(\"stop_lon\"), col(\"lat_wgs84\").cast(DoubleType()), col(\"lon_wgs84\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Using window function to find the nearest station\n",
    "windowSpec = Window.partitionBy(\"stops_stop_id\").orderBy(\"distance\")\n",
    "nearest_stations = distance_df.withColumn(\"rank\", row_number().over(windowSpec)) \\\n",
    "                              .filter(col(\"rank\") == 1) \\\n",
    "                              .drop(\"rank\")\\\n",
    "                              .select(['stops_stop_id', 'site', 'lat_wgs84', 'lon_wgs84', 'distance'])\n",
    "\n",
    "# Show the results\n",
    "nearest_stations.show(5)\n",
    "nearest_stations.printSchema()\n",
    "nearest_stations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da3269-4168-494f-aadb-0382ebc6d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_df = spark.read.orc('/data/share/weather_win3h_avg_df.orc')\n",
    "\n",
    "weather_data_df.printSchema()\n",
    "weather_data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7865d45-3306-44ee-923a-207886df3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, to_timestamp, round, from_unixtime, hour\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "weather_data_df = spark.read.orc('/data/share/weather_win3h_avg_df.orc')\n",
    "\n",
    "# Merge weather data with station coordinates\n",
    "weather_stations_df = weather_data_df.join(\n",
    "    nearest_stations,\n",
    "    weather_data_df.site == nearest_stations.site,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Select necessary columns including coordinates in WGS84 which will help in geographical matching\n",
    "weather_stations_df = weather_stations_df.select(\n",
    "    \"valid_time_gmt\", \"year\", \"month\", \"dayofmonth\", \"hour\", \n",
    "    \"temp\", \"avg_temp\", \"lat_wgs84\", \"lon_wgs84\", \"stops_stop_id\"\n",
    ").withColumn(\"weather_hour\", (col(\"valid_time_gmt\") / 3600).cast(\"integer\") * 3600)\n",
    "\n",
    "istdaten_df = istdaten_df.withColumn(\"arrival_unix\", unix_timestamp(to_timestamp(col(\"arrival_time\"))))\\\n",
    "                         .withColumn(\"arrival_unix\", (col(\"arrival_unix\") / 3600).cast(\"integer\") * 3600)\n",
    "\n",
    "istdaten_df = istdaten_df.join(\n",
    "    weather_stations_df,\n",
    "    (istdaten_df.stop_id == weather_stations_df.stops_stop_id) &\n",
    "    (istdaten_df.arrival_unix == weather_stations_df.weather_hour),\n",
    "    \"left_outer\"  # left_outer to keep transport data regardless of match\n",
    ")\n",
    "\n",
    "istdaten_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df76e7-d17a-449b-85d4-def4ce2fa51f",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905cc891-211f-45b8-b687-1e681b2940ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+-------------+----+-----+---+-----------+----------+------------+----------------+----------------+-------------+--------------+------------------+---------------------+------------+\n",
      "|stop_id|transportation|       arrival_time|delay_arrival|year|month|day|day_of_week|is_weekend|is_peak_time|        stop_lat|        stop_lon|trip_stop_seq|trip_num_stops|traveling_time_min|traveling_distance_km|arrival_hour|\n",
      "+-------+--------------+-------------------+-------------+----+-----+---+-----------+----------+------------+----------------+----------------+-------------+--------------+------------------+---------------------+------------+\n",
      "|8500090|           Zug|2023-01-05 17:23:00|          540|2023|    1|  5|          3|     false|        true|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|          17|\n",
      "|8500090|           Zug|2023-01-07 13:50:00|          240|2023|    1|  7|          5|      true|       false|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|          13|\n",
      "|8500090|           Zug|2023-01-10 13:08:00|            0|2023|    1| 10|          1|     false|       false|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|          13|\n",
      "|8500090|           Zug|2023-01-12 17:38:00|            0|2023|    1| 12|          3|     false|        true|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|          17|\n",
      "|8500090|           Zug|2023-10-11 05:56:00|          120|2023|   10| 11|          2|     false|       false|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|           5|\n",
      "+-------+--------------+-------------------+-------------+----+-----+---+-----------+----------+------------+----------------+----------------+-------------+--------------+------------------+---------------------+------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "features = istdaten_df.select(['stop_id','transportation','arrival_time','delay_arrival','year','month','day','day_of_week','is_weekend','is_peak_time',\n",
    "                               'stop_lat','stop_lon','trip_stop_seq','trip_num_stops','traveling_time_min','traveling_distance_km'])\\\n",
    "                      .withColumn(\"arrival_hour\", hour(col(\"arrival_time\")))\\\n",
    "\n",
    "features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea63631f-b4f6-4ea0-b05a-e3b8359707e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+-------------+----+-----+---+-----------+----------+------------+----------------+----------------+-------------+--------------+------------------+---------------------+------------+------------------+--------------------+-----------------------+-------------------------+--------------------------+----------------------------+\n",
      "|stop_id|transportation|       arrival_time|delay_arrival|year|month|day|day_of_week|is_weekend|is_peak_time|        stop_lat|        stop_lon|trip_stop_seq|trip_num_stops|traveling_time_min|traveling_distance_km|arrival_hour|mean_delay_arrival|stddev_delay_arrival|mean_traveling_time_min|stddev_traveling_time_min|mean_traveling_distance_km|stddev_traveling_distance_km|\n",
      "+-------+--------------+-------------------+-------------+----+-----+---+-----------+----------+------------+----------------+----------------+-------------+--------------+------------------+---------------------+------------+------------------+--------------------+-----------------------+-------------------------+--------------------------+----------------------------+\n",
      "|8500090|           Zug|2023-01-12 10:02:00|            0|2023|    1| 12|          3|     false|       false|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|          10| 250.9250721318302|   650.9831396824119|      42.50823401950163|       34.257593533797674|          43.4580355482178|          38.258216989475606|\n",
      "|8500090|           Zug|2023-01-05 12:08:00|           60|2023|    1|  5|          3|     false|       false|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|          12| 250.9250721318302|   650.9831396824119|      42.50823401950163|       34.257593533797674|          43.4580355482178|          38.258216989475606|\n",
      "|8500090|           Zug|2023-05-16 05:56:00|          120|2023|    5| 16|          1|     false|       false|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|           5| 250.9250721318302|   650.9831396824119|      42.50823401950163|       34.257593533797674|          43.4580355482178|          38.258216989475606|\n",
      "|8500090|           Zug|2023-07-25 07:03:00|          120|2023|    7| 25|          1|     false|        true|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|           7| 250.9250721318302|   650.9831396824119|      42.50823401950163|       34.257593533797674|          43.4580355482178|          38.258216989475606|\n",
      "|8500090|           Zug|2023-03-20 07:39:00|           60|2023|    3| 20|          0|     false|        true|47.5673090682292|7.60691586067732|            1|             1|              null|                 null|           7| 250.9250721318302|   650.9831396824119|      42.50823401950163|       34.257593533797674|          43.4580355482178|          38.258216989475606|\n",
      "+-------+--------------+-------------------+-------------+----+-----+---+-----------+----------+------------+----------------+----------------+-------------+--------------+------------------+---------------------+------------+------------------+--------------------+-----------------------+-------------------------+--------------------------+----------------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Compute mean and std deviation for relevant columns per stop_id\n",
    "statistics_df = features.groupBy('stop_id').agg(\n",
    "    F.mean('delay_arrival').alias('mean_delay_arrival'),\n",
    "    F.stddev('delay_arrival').alias('stddev_delay_arrival'),\n",
    "    F.mean('traveling_time_min').alias('mean_traveling_time_min'),\n",
    "    F.stddev('traveling_time_min').alias('stddev_traveling_time_min'),\n",
    "    F.mean('traveling_distance_km').alias('mean_traveling_distance_km'),\n",
    "    F.stddev('traveling_distance_km').alias('stddev_traveling_distance_km')\n",
    ")\n",
    "\n",
    "# Join the statistics back to the original dataframe\n",
    "features_with_stats = features.join(statistics_df, on='stop_id', how='left')\n",
    "\n",
    "# Show the result\n",
    "features_with_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d89772-9a06-43de-b879-408154f0ad44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Select features and target variable\n",
    "feature_columns = ['transportation', 'year', 'month', 'day', 'day_of_week', 'is_weekend', 'is_peak_time', \n",
    "                   'stop_lat', 'stop_lon', 'trip_stop_seq', 'trip_num_stops', 'traveling_time_min', \n",
    "                   'traveling_distance_km', 'arrival_hour', 'mean_delay_arrival', 'stddev_delay_arrival', \n",
    "                   'mean_traveling_time_min', 'stddev_traveling_time_min', 'mean_traveling_distance_km', \n",
    "                   'stddev_traveling_distance_km']\n",
    "target_column = 'delay_arrival'\n",
    "\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "# Index the 'transportation' column\n",
    "indexer = StringIndexer(inputCol=\"transportation\", outputCol=\"transportation_indexed\")\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=['transportation_indexed'] + feature_columns[1:], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d84ed8d-9994-4853-96fc-7c12b7491b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o618.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 83 in stage 254.0 failed 4 times, most recent failure: Lost task 83.3 in stage 254.0 (TID 13013) (iccluster065.iccluster.epfl.ch executor 639): ExecutorLostFailure (executor 639 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding physical memory limits. 5.1 GB of 5 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 383, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 380, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o618.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 83 in stage 254.0 failed 4 times, most recent failure: Lost task 83.3 in stage 254.0 (TID 13013) (iccluster065.iccluster.epfl.ch executor 639): ExecutorLostFailure (executor 639 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding physical memory limits. 5.1 GB of 5 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with null values\n",
    "features_with_stats = features_with_stats.na.drop()\n",
    "\n",
    "# Sample the data for initial model training and testing --> no sampling leads to crash (out of memory)\n",
    "# Can be solved in two ways:\n",
    "# 1. Providing additional resources -> expanding RAM and number of cores\n",
    "# 2. Data processing -> filtering on select stops to remove less relevant rows\n",
    "sampled_data = features_with_stats.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "\n",
    "# Define the Random Forest Regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target_column)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, assembler, rf])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = sampled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb74f64-9a5e-479d-b076-4024d0d82cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Show some predictions\n",
    "predictions.select(\"stop_id\", \"arrival_time\", target_column, \"prediction\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
