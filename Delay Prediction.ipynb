{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272b589b-87b8-4d1b-a91d-9bb8f54609e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorMemory': '4G', 'executorCores': 4, 'numExecutors': 10, 'conf': {'spark.executorEnv.USERNAME': 'kvaerum', 'spark.executorEnv.HADOOP_DEFAULT_FS': 'hdfs://iccluster067.iccluster.epfl.ch:8020'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3735</td><td>application_1713270977862_4021</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4021/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4021_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3824</td><td>application_1713270977862_4118</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4118/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster077.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4118_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3829</td><td>application_1713270977862_4123</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4123/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4123_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3830</td><td>application_1713270977862_4124</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4124/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4124_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3831</td><td>application_1713270977862_4125</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4125/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4125_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31298665-7d1a-48a4-bdbd-f88f4c5ca842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Spark name:livy-session-3831, version:3.3.2.3.3.7190.2-1"
     ]
    }
   ],
   "source": [
    "print(f'Start Spark name:{spark._sc.appName}, version:{spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389b902d-c1ad-4373-8396-acad795fdb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local username=kvaerum\n",
      "hadoop_fs=hdfs://iccluster067.iccluster.epfl.ch:8020\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import os\n",
    "default_db = 'com490'\n",
    "username=os.getenv('USER', 'anonymous')\n",
    "hadoop_fs=os.getenv('HADOOP_DEFAULT_FS', 'hdfs://iccluster067.iccluster.epfl.ch:8020')\n",
    "print(f\"local username={username}\\nhadoop_fs={hadoop_fs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d892673-ce68-4907-8d9e-545553636ecd",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "23d2dc77-4db5-4279-9915-356d15a8e1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://iccluster067.iccluster.epfl.ch:8020/data/sbb/csv/timetables/stops/year=2020\n",
      "hdfs://iccluster067.iccluster.epfl.ch:8020/data/sbb/csv/timetables/stops/year=2021\n",
      "hdfs://iccluster067.iccluster.epfl.ch:8020/data/sbb/csv/timetables/stops/year=2022\n",
      "hdfs://iccluster067.iccluster.epfl.ch:8020/data/sbb/csv/timetables/stops/year=2023\n",
      "hdfs://iccluster067.iccluster.epfl.ch:8020/data/sbb/csv/timetables/stops/year=2024"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# List files in the directory\n",
    "files = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration()) \\\n",
    "    .listStatus(sc._jvm.org.apache.hadoop.fs.Path('/data/sbb/csv/timetables/stops'))\n",
    "for file_status in files:\n",
    "    print(file_status.getPath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e9ecb-f013-4f92-abef-04643d0abb83",
   "metadata": {},
   "source": [
    "## Istdaten Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bbeabc-c7da-4c97-865a-94a066673d18",
   "metadata": {},
   "source": [
    "### 1) Select Relevant Columns and Rename for Convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f3316c4c-4dee-43ab-8a94-f9917cb5f7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------+-------+--------------+---------+----------------+-------------------+---------------+----------------+-------------------+---------------+\n",
      "|      date|       trip_id|operator_id|stop_id|transportation|cancelled|    arrival_time|  arrival_prognosis|arr_prog_status|  departure_time|departure_prognosis|dep_prog_status|\n",
      "+----------+--------------+-----------+-------+--------------+---------+----------------+-------------------+---------------+----------------+-------------------+---------------+\n",
      "|22.12.2023|85:65:8031:001|      85:65|8506101|           Zug|    false|22.12.2023 09:36|22.12.2023 09:36:34|           REAL|22.12.2023 09:36|22.12.2023 09:36:53|           REAL|\n",
      "|22.12.2023|85:65:8031:001|      85:65|8506102|           Zug|    false|22.12.2023 09:39|22.12.2023 09:39:08|           REAL|22.12.2023 09:39|22.12.2023 09:39:26|           REAL|\n",
      "|22.12.2023|85:65:8031:001|      85:65|8506103|           Zug|    false|22.12.2023 09:42|22.12.2023 09:42:27|           REAL|22.12.2023 09:42|22.12.2023 09:43:48|           REAL|\n",
      "|22.12.2023|85:65:8031:001|      85:65|8506104|           Zug|    false|22.12.2023 09:45|22.12.2023 09:46:15|           REAL|22.12.2023 09:45|22.12.2023 09:46:40|           REAL|\n",
      "|22.12.2023|85:65:8031:001|      85:65|8506105|           Zug|    false|22.12.2023 09:50|22.12.2023 09:50:40|           REAL|                |                   |               |\n",
      "+----------+--------------+-----------+-------+--------------+---------+----------------+-------------------+---------------+----------------+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- arrival_prognosis: string (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- departure_prognosis: string (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      "\n",
      "763642603"
     ]
    }
   ],
   "source": [
    "istdaten_df = spark.read.orc('/data/sbb/orc/istdaten/year=2023')\n",
    "\n",
    "istdaten_df = istdaten_df.select([\n",
    "  istdaten_df['betriebstag'].alias('date'),\n",
    "  istdaten_df['fahrt_bezeichner'].alias('trip_id'),\n",
    "  istdaten_df['betreiber_id'].alias('operator_id'),\n",
    "  istdaten_df['bpuic'].alias('stop_id'),\n",
    "  istdaten_df['produkt_id'].alias('transportation'),\n",
    "  istdaten_df['faellt_aus_tf'].alias('cancelled'),\n",
    "  istdaten_df['ankunftszeit'].alias('arrival_time'),\n",
    "  istdaten_df['an_prognose'].alias('arrival_prognosis'),\n",
    "  istdaten_df['an_prognose_status'].alias('arr_prog_status'),\n",
    "  istdaten_df['abfahrtszeit'].alias('departure_time'),\n",
    "  istdaten_df['ab_prognose'].alias('departure_prognosis'),\n",
    "  istdaten_df['ab_prognose_status'].alias('dep_prog_status'),\n",
    "])\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efc8cc-4646-4377-823b-42680e4eb955",
   "metadata": {},
   "source": [
    "### 2) Filtering and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9234fae2-fcce-4a51-b0d6-5823c2a3ceea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GESCHAETZT', 'PROGNOSE', 'REAL']"
     ]
    }
   ],
   "source": [
    "distinct_arr_prog_status = istdaten_df.select(\"arr_prog_status\").distinct().collect()\n",
    "distinct_arr_prog_status_list = [row.arr_prog_status for row in distinct_arr_prog_status]\n",
    "print(distinct_arr_prog_status_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0c86eeb2-6cce-42f2-a363-b998fb1de332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "|               date|       trip_id|operator_id|stop_id|transportation|cancelled|       arrival_time|  arrival_prognosis|arr_prog_status|     departure_time|departure_prognosis|dep_prog_status|delay_arrival|delay_departure|delay_at_station|year|month|day|day_of_week|is_weekday|is_weekend|is_peak_time|\n",
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506101|           Zug|    false|2023-12-22 09:36:00|2023-12-22 09:36:34|           REAL|2023-12-22 09:36:00|2023-12-22 09:36:53|           REAL|           34|             53|              19|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506102|           Zug|    false|2023-12-22 09:39:00|2023-12-22 09:39:08|           REAL|2023-12-22 09:39:00|2023-12-22 09:39:26|           REAL|            8|             26|              18|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506103|           Zug|    false|2023-12-22 09:42:00|2023-12-22 09:42:27|           REAL|2023-12-22 09:42:00|2023-12-22 09:43:48|           REAL|           27|            108|              81|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506104|           Zug|    false|2023-12-22 09:45:00|2023-12-22 09:46:15|           REAL|2023-12-22 09:45:00|2023-12-22 09:46:40|           REAL|           75|            100|              25|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506105|           Zug|    false|2023-12-22 09:50:00|2023-12-22 09:50:40|           REAL|               null|               null|               |           40|           null|            null|2023|   12| 22|          4|      true|     false|       false|\n",
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- arrival_prognosis: timestamp (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- departure_prognosis: timestamp (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      " |-- delay_arrival: long (nullable = true)\n",
      " |-- delay_departure: long (nullable = true)\n",
      " |-- delay_at_station: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_peak_time: boolean (nullable = false)\n",
      "\n",
      "705684079"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "format_date = \"dd.MM.yyyy\"\n",
    "format_timetable = \"dd.MM.yyyy HH:mm\"\n",
    "format_prognosis = \"dd.MM.yyyy HH:mm:ss\"\n",
    "\n",
    "istdaten_df = istdaten_df.withColumn(\"date\", to_timestamp(col(\"date\"), format_date))\\\n",
    "                         .withColumn(\"arrival_time\", to_timestamp(col(\"arrival_time\"), format_timetable))\\\n",
    "                         .withColumn(\"arrival_prognosis\", to_timestamp(col(\"arrival_prognosis\"), format_prognosis))\\\n",
    "                         .withColumn(\"departure_time\", to_timestamp(col(\"departure_time\"), format_timetable))\\\n",
    "                         .withColumn(\"departure_prognosis\", to_timestamp(col(\"departure_prognosis\"), format_prognosis))\n",
    "\n",
    "istdaten_df = istdaten_df.filter((col(\"arr_prog_status\") != \"\") & (col(\"arr_prog_status\") != \"UNBEKANNT\") & (col('cancelled') == 'false'))\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e5656-ba7e-433c-9994-8eec3b7f6db1",
   "metadata": {},
   "source": [
    "### 3) Istdaten Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3fcb7ba9-920e-4a00-b384-6d28979230bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "|               date|       trip_id|operator_id|stop_id|transportation|cancelled|       arrival_time|  arrival_prognosis|arr_prog_status|     departure_time|departure_prognosis|dep_prog_status|delay_arrival|delay_departure|delay_at_station|year|month|day|day_of_week|is_weekday|is_weekend|is_peak_time|\n",
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506101|           Zug|    false|2023-12-22 09:36:00|2023-12-22 09:36:34|           REAL|2023-12-22 09:36:00|2023-12-22 09:36:53|           REAL|           34|             53|              19|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506102|           Zug|    false|2023-12-22 09:39:00|2023-12-22 09:39:08|           REAL|2023-12-22 09:39:00|2023-12-22 09:39:26|           REAL|            8|             26|              18|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506103|           Zug|    false|2023-12-22 09:42:00|2023-12-22 09:42:27|           REAL|2023-12-22 09:42:00|2023-12-22 09:43:48|           REAL|           27|            108|              81|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506104|           Zug|    false|2023-12-22 09:45:00|2023-12-22 09:46:15|           REAL|2023-12-22 09:45:00|2023-12-22 09:46:40|           REAL|           75|            100|              25|2023|   12| 22|          4|      true|     false|       false|\n",
      "|2023-12-22 00:00:00|85:65:8031:001|      85:65|8506105|           Zug|    false|2023-12-22 09:50:00|2023-12-22 09:50:40|           REAL|               null|               null|               |           40|           null|            null|2023|   12| 22|          4|      true|     false|       false|\n",
      "+-------------------+--------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- arrival_prognosis: timestamp (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- departure_prognosis: timestamp (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      " |-- delay_arrival: long (nullable = true)\n",
      " |-- delay_departure: long (nullable = true)\n",
      " |-- delay_at_station: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_peak_time: boolean (nullable = false)\n",
      "\n",
      "705684079"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, month, dayofmonth, dayofweek, expr, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "istdaten_df = istdaten_df.withColumn(\"delay_arrival\", (col(\"arrival_prognosis\").cast(\"long\") - col(\"arrival_time\").cast(\"long\")))\\\n",
    "                         .withColumn(\"delay_departure\", (col(\"departure_prognosis\").cast(\"long\") - col(\"departure_time\").cast(\"long\")))\\\n",
    "                         .withColumn(\"delay_at_station\", (col(\"delay_departure\") - col(\"delay_arrival\")))\\\n",
    "                         .withColumn(\"year\", year(col(\"date\")))\\\n",
    "                         .withColumn(\"month\", month(col(\"date\")))\\\n",
    "                         .withColumn(\"day\", dayofmonth(col(\"date\")))\\\n",
    "                         .withColumn(\"day_of_week\", (dayofweek(col(\"date\")) + 5) % 7)\\\n",
    "                         .withColumn(\"is_weekday\", (col(\"day_of_week\") <= 4))\\\n",
    "                         .withColumn(\"is_weekend\", (col(\"day_of_week\") > 4))\\\n",
    "                         .withColumn(\"is_peak_time\",\n",
    "                                     when((col(\"is_weekday\")) & \n",
    "                                          ((col(\"arrival_time\").between(expr(\"make_timestamp(year(date), month(date), day(date), 6, 30, 0)\"), \n",
    "                                                                        expr(\"make_timestamp(year(date), month(date), day(date), 8, 30, 0)\"))) |\n",
    "                                           (col(\"arrival_time\").between(expr(\"make_timestamp(year(date), month(date), day(date), 16, 30, 0)\"), \n",
    "                                                                        expr(\"make_timestamp(year(date), month(date), day(date), 18, 30, 0)\")))), True)\n",
    "                                     .otherwise(False))\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd210b-4c5b-493d-8634-fb208efe803f",
   "metadata": {},
   "source": [
    "### 4) Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "69e7e6e0-bc66-44a6-91b3-66f9ad8e84f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+\n",
      "|               date|             trip_id|operator_id|stop_id|transportation|cancelled|       arrival_time|  arrival_prognosis|arr_prog_status|     departure_time|departure_prognosis|dep_prog_status|delay_arrival|delay_departure|delay_at_station|year|month|day|day_of_week|is_weekday|is_weekend|is_peak_time|        stop_lat|        stop_lon|\n",
      "+-------------------+--------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+\n",
      "|2023-03-23 00:00:00|85:151:TL070-4506...|     85:151|8530749|         Metro|    false|2023-03-23 10:09:00|2023-03-23 10:11:17|       PROGNOSE|               null|               null|               |          137|           null|            null|2023|    3| 23|          3|      true|     false|       false|46.5374792630591|6.57807536563201|\n",
      "|2023-03-18 00:00:00|85:766:202055-621...|     85:766|8570850|           Bus|    false|2023-03-18 20:40:00|2023-03-18 20:43:30|           REAL|               null|               null|               |          210|           null|            null|2023|    3| 18|          5|     false|      true|       false|46.8756220371815|9.52330982286194|\n",
      "|2023-07-04 00:00:00|85:773:67276-14787-1|     85:773|8580432|           Bus|    false|2023-07-04 16:36:00|2023-07-04 16:37:23|           REAL|2023-07-04 16:36:00|2023-07-04 16:38:16|           REAL|           83|            136|              53|2023|    7|  4|          1|      true|     false|        true|47.4439106426415|8.57842587428213|\n",
      "|2023-11-06 00:00:00|85:151:TL031-4506...|     85:151|8588156|           Bus|    false|2023-11-06 18:39:00|2023-11-06 18:40:29|       PROGNOSE|2023-11-06 18:39:00|2023-11-06 18:40:45|       PROGNOSE|           89|            105|              16|2023|   11|  6|          0|      true|     false|       false|46.5112787626392|6.55061386739266|\n",
      "|2023-11-06 00:00:00|85:151:TL031-4506...|     85:151|8588156|           Bus|    false|2023-11-06 19:49:00|2023-11-06 19:49:00|       PROGNOSE|2023-11-06 19:49:00|2023-11-06 19:49:00|       PROGNOSE|            0|              0|               0|2023|   11|  6|          0|      true|     false|       false|46.5112787626392|6.55061386739266|\n",
      "+-------------------+--------------------+-----------+-------+--------------+---------+-------------------+-------------------+---------------+-------------------+-------------------+---------------+-------------+---------------+----------------+----+-----+---+-----------+----------+----------+------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- transportation: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- arrival_prognosis: timestamp (nullable = true)\n",
      " |-- arr_prog_status: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- departure_prognosis: timestamp (nullable = true)\n",
      " |-- dep_prog_status: string (nullable = true)\n",
      " |-- delay_arrival: long (nullable = true)\n",
      " |-- delay_departure: long (nullable = true)\n",
      " |-- delay_at_station: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_peak_time: boolean (nullable = false)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      "\n",
      "705684079"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "stops_df = spark.read.csv('/data/sbb/csv/timetables/stops', header=True)\\\n",
    "                .select(['stop_id','stop_lat', 'stop_lon'])\\\n",
    "                .drop(*['year','month','day'])\\\n",
    "                .dropDuplicates(['stop_id'])\\\n",
    "                .withColumnRenamed(\"stop_id\", \"stops_stop_id\")\\\n",
    "                .withColumn(\"stop_lat\", col(\"stop_lat\").cast(DoubleType()))\\\n",
    "                .withColumn(\"stop_lon\", col(\"stop_lon\").cast(DoubleType()))\n",
    "\n",
    "istdaten_df = istdaten_df.join(\n",
    "    stops_df,\n",
    "    istdaten_df.stop_id == stops_df.stops_stop_id,\n",
    "    \"left\"\n",
    ").drop('stops_stop_id')\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2e03bd11-8fad-4f5a-9005-8daf6c81da0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------+-------------+-----------+-------------+-----+---+----+----------------+----------------+---------------+------------+-------------------+----------------+----------------+------------------+---------------------+\n",
      "|             trip_id|       arrival_time|     departure_time|stop_id|stop_sequence|pickup_type|drop_off_type|month|day|year|        stop_lat|        stop_lon|number_of_stops|prev_stop_id|prev_departure_time|   prev_stop_lat|   prev_stop_lon|traveling_time_min|traveling_distance_km|\n",
      "+--------------------+-------------------+-------------------+-------+-------------+-----------+-------------+-----+---+----+----------------+----------------+---------------+------------+-------------------+----------------+----------------+------------------+---------------------+\n",
      "|1.TA.91-10-j23-1.1.H|2023-08-23 19:42:00|2023-08-23 19:42:00|8500065|            1|          0|            0|   08| 23|2023|47.4837805948106|7.54601008440553|              4|        null|               null|            null|            null|              null|                 null|\n",
      "|1.TA.91-10-j23-1.1.H|2023-08-23 19:44:00|2023-08-23 19:44:00|8588795|            2|          0|            0|   08| 23|2023| 47.495083217035|7.55468781005134|              4|     8500065|2023-08-23 19:42:00|47.4837805948106|7.54601008440553|               2.0|   1.4158613162755302|\n",
      "|1.TA.91-10-j23-1.1.H|2023-08-23 19:45:00|2023-08-23 19:45:00|8500064|            3|          0|            0|   08| 23|2023|47.4993316940542|7.55717614338869|              4|     8588795|2023-08-23 19:44:00| 47.495083217035|7.55468781005134|               1.0|   0.5080517179755902|\n",
      "|1.TA.91-10-j23-1.1.H|2023-08-23 19:47:00|2023-08-23 19:47:00|8500072|            4|          0|            0|   08| 23|2023|47.5065411797304| 7.5552717149861|              4|     8500064|2023-08-23 19:45:00|47.4993316940542|7.55717614338869|               2.0|   0.8143225302094911|\n",
      "|1.TA.91-10-j23-1.1.H|2023-10-18 19:57:00|2023-10-18 19:57:00|8500065|            1|          0|            0|   10| 18|2023|47.4837805948106|7.54601008440553|              4|        null|               null|            null|            null|              null|                 null|\n",
      "+--------------------+-------------------+-------------------+-------+-------------+-----------+-------------+-----+---+----+----------------+----------------+---------------+------------+-------------------+----------------+----------------+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- stop_sequence: string (nullable = true)\n",
      " |-- pickup_type: string (nullable = true)\n",
      " |-- drop_off_type: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      " |-- number_of_stops: long (nullable = false)\n",
      " |-- prev_stop_id: string (nullable = true)\n",
      " |-- prev_departure_time: timestamp (nullable = true)\n",
      " |-- prev_stop_lat: double (nullable = true)\n",
      " |-- prev_stop_lon: double (nullable = true)\n",
      " |-- traveling_time_min: double (nullable = true)\n",
      " |-- traveling_distance_km: double (nullable = true)\n",
      "\n",
      "778828189"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, count, lag, col, to_timestamp, concat_ws, lit, lpad\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "# Haversine formula to calculate the distance between two points on the earth\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    if lat1 is None or lon1 is None or lat2 is None or lon2 is None:\n",
    "        return None\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c  # Distance in kilometers\n",
    "    return distance\n",
    "\n",
    "# Register the UDF\n",
    "haversine_udf = udf(haversine, DoubleType())\n",
    "\n",
    "\n",
    "stop_times_df = spark.read.csv('/data/sbb/csv/timetables/stop_times/year=2023', header=True)\\\n",
    "                             .withColumn(\"year\", lit(2023))\\\n",
    "                             .withColumn(\"month\", lpad(col(\"month\").cast(\"string\"), 2, \"0\"))\\\n",
    "                             .withColumn(\"day\", lpad(col(\"day\").cast(\"string\"), 2, \"0\"))\\\n",
    "                             .withColumn(\"arrival_time\", concat_ws(\" \", concat_ws(\"-\", col(\"year\"), col(\"month\"), col(\"day\")), col(\"arrival_time\")))\\\n",
    "                             .withColumn(\"departure_time\", concat_ws(\" \", concat_ws(\"-\", col(\"year\"), col(\"month\"), col(\"day\")), col(\"departure_time\")))\\\n",
    "                             .withColumn(\"arrival_time\", to_timestamp(col(\"arrival_time\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "                             .withColumn(\"departure_time\", to_timestamp(col(\"departure_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "stop_times_df = stop_times_df.join(\n",
    "    stops_df,\n",
    "    stop_times_df.stop_id == stops_df.stops_stop_id,\n",
    "    \"left\"\n",
    ").drop('stops_stop_id')\n",
    "\n",
    "# Add a column representing the number of stops for each trip_id\n",
    "window_spec_trip = Window.partitionBy(\"trip_id\", \"year\", \"month\", \"day\")\n",
    "stop_times_df = stop_times_df.withColumn(\"number_of_stops\", count(\"stop_id\").over(window_spec_trip))\n",
    "\n",
    "# Add columns for the previous stop_id and departure_time\n",
    "window_spec_seq = Window.partitionBy(\"trip_id\", \"year\", \"month\", \"day\").orderBy(\"stop_sequence\")\n",
    "stop_times_df = stop_times_df.withColumn(\"prev_stop_id\", lag(\"stop_id\", 1).over(window_spec_seq))\\\n",
    "                             .withColumn(\"prev_departure_time\", lag(\"departure_time\", 1).over(window_spec_seq))\\\n",
    "                             .withColumn(\"prev_stop_lat\", lag(\"stop_lat\", 1).over(window_spec_seq))\\\n",
    "                             .withColumn(\"prev_stop_lon\", lag(\"stop_lon\", 1).over(window_spec_seq))\\\n",
    "                             .withColumn(\"traveling_time_min\", ((col(\"arrival_time\").cast(\"long\") - col(\"prev_departure_time\").cast(\"long\"))) / 60)\\\n",
    "                             .withColumn(\"traveling_distance_km\", haversine_udf(col(\"prev_stop_lat\"), col(\"prev_stop_lon\"),col(\"stop_lat\"), col(\"stop_lon\")))\n",
    " \n",
    "stop_times_df.show(5)\n",
    "stop_times_df.printSchema()\n",
    "stop_times_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3427e138-8e21-45ae-bc44-fd990d2fcebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "right = stop_times_df.select(['trip_id', 'stop_id', 'arrival_time', 'stop_sequence', 'number_of_stops', 'traveling_time_min', 'traveling_distance_km'])\\\n",
    "                            .dropDuplicates(['trip_id', 'stop_id', 'arrival_time'])\n",
    "left = istdaten_df.select(['date','stop_id', 'transportation', 'trip_id', 'delay_arrival', 'delay_departure', 'delay_at_station', 'day_of_week', 'is_weekend', 'is_peak_time', 'stop_lat', 'stop_lon'])\n",
    "\n",
    "merge = left.join(\n",
    "    right,\n",
    "    (left.trip_id == right.trip_id) & (left.stop_id == right.stop_id) & (left.date.cast('date') == right.arrival_time.cast('date')),\n",
    "    \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4aa289d9-4aa0-482a-b902-00b37f02fc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+-------+-------+------------+-------------+---------------+------------------+---------------------+\n",
      "|               date|stop_id|transportation|             trip_id|delay_arrival|delay_departure|delay_at_station|day_of_week|is_weekend|is_peak_time|        stop_lat|        stop_lon|trip_id|stop_id|arrival_time|stop_sequence|number_of_stops|traveling_time_min|traveling_distance_km|\n",
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+-------+-------+------------+-------------+---------------+------------------+---------------------+\n",
      "|2023-12-30 00:00:00|8506290|           Zug|      85:22:1073:000|          -77|             74|             151|          5|      true|       false|47.3901644308777|9.27681210886523|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-01-28 00:00:00|8503495|           Bus|85:836:293606-007...|          -44|              0|              44|          5|      true|       false|47.6978606462172| 8.6327469995204|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-05-12 00:00:00|8502362|           BUS|        85:723:185-1|          118|            123|               5|          4|     false|       false|47.3147837096966|7.92740780463725|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-07-03 00:00:00|8503150|           Bus|85:773:64047-01720-1|           75|           null|            null|          0|     false|        true|47.3895866742558|8.69322158445574|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-08-06 00:00:00|8570168|           Bus|85:151:TL060-4506...|          149|            180|              31|          6|      true|       false|46.6012310261959|6.67937838023627|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-02-10 00:00:00|8508134|          Tram|85:881:TPG014-450...|         -107|           null|            null|          4|     false|       false|46.1749963991528|6.06076254289409|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-12-30 00:00:00|8506284|           Zug|      85:22:1073:000|           75|            101|              26|          5|      true|       false|   47.3200820123|9.32817777681833|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-03-12 00:00:00|8507733|           Bus|    85:801:61136-757|          294|            298|               4|          6|      true|       false|46.7758529169904| 7.3840348555034|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-09-10 00:00:00|8502593|           Bus|   85:801:61649-1571|            0|              0|               0|          6|      true|       false|47.2847987095341|8.09782719721129|   null|   null|        null|         null|           null|              null|                 null|\n",
      "|2023-10-17 00:00:00|8506675|           Bus|   85:801:73062-2282|          177|            199|              22|          1|     false|       false|47.4247939279425|9.08627045392411|   null|   null|        null|         null|           null|              null|                 null|\n",
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+-------+-------+------------+-------------+---------------+------------------+---------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "merge.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df93f39-3c3b-471c-b859-316abefac105",
   "metadata": {},
   "source": [
    "### TODO: Figure Out How to Merge Istdaten with other tables. Currently trip_id mismatch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "21ddf314-b051-498d-bab6-894ee0de9d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+-------------+---------------+------------------+---------------------+\n",
      "|             trip_id|     stop_id|       arrival_time|stop_sequence|number_of_stops|traveling_time_min|traveling_distance_km|\n",
      "+--------------------+------------+-------------------+-------------+---------------+------------------+---------------------+\n",
      "|1.TA.91-10-j23-1.1.H|     8500065|2023-08-23 19:42:00|            1|              4|              null|                 null|\n",
      "|1.TA.91-10-j23-1.1.H|     8588795|2023-08-23 19:44:00|            2|              4|               2.0|   1.4158613162755302|\n",
      "|1.TA.91-10-j23-1.1.H|     8500064|2023-08-23 19:45:00|            3|              4|               1.0|   0.5080517179755902|\n",
      "|1.TA.91-10-j23-1.1.H|     8500072|2023-08-23 19:47:00|            4|              4|               2.0|   0.8143225302094911|\n",
      "|1.TA.91-10-j23-1.1.H|     8500065|2023-10-18 19:57:00|            1|              4|              null|                 null|\n",
      "|1.TA.91-10-j23-1.1.H|     8588795|2023-10-18 19:59:00|            2|              4|               2.0|   1.4158613162755302|\n",
      "|1.TA.91-10-j23-1.1.H|     8500064|2023-10-18 20:00:00|            3|              4|               1.0|   0.5080517179755902|\n",
      "|1.TA.91-10-j23-1.1.H|     8500072|2023-10-18 20:02:00|            4|              4|               2.0|   0.8143225302094911|\n",
      "|1.TA.91-11-G-j23-...|8509000:0:11|2023-07-05 14:28:00|            1|              6|              null|                 null|\n",
      "|1.TA.91-11-G-j23-...| 8509181:0:2|2023-07-05 14:34:00|            2|              6|               6.0|    6.175070661732204|\n",
      "+--------------------+------------+-------------------+-------------+---------------+------------------+---------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "stop_times_df.select(['trip_id', 'stop_id', 'arrival_time', 'stop_sequence', 'number_of_stops', 'traveling_time_min', 'traveling_distance_km']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9b892060-e143-4dbb-953a-60204998769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+\n",
      "|               date|stop_id|transportation|             trip_id|delay_arrival|delay_departure|delay_at_station|day_of_week|is_weekend|is_peak_time|        stop_lat|        stop_lon|\n",
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+\n",
      "|2023-10-11 00:00:00|8589289|           Bus|85:823:36715-00009-1|           61|              9|             -52|          2|     false|       false|47.5528391160685|7.55672698574657|\n",
      "|2023-11-06 00:00:00|8588156|           Bus|85:151:TL031-4506...|           89|            105|              16|          0|     false|       false|46.5112787626392|6.55061386739266|\n",
      "|2023-11-06 00:00:00|8588156|           Bus|85:151:TL031-4506...|            0|              0|               0|          0|     false|       false|46.5112787626392|6.55061386739266|\n",
      "|2023-11-03 00:00:00|8592065|           Bus|85:151:TL042-4506...|          103|            109|               6|          4|     false|       false|46.5302551451146|6.65707321172848|\n",
      "|2023-03-23 00:00:00|8530749|         Metro|85:151:TL070-4506...|          137|           null|            null|          3|     false|       false|46.5374792630591|6.57806638247917|\n",
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "istdaten_df.select(['date','stop_id', 'transportation', 'trip_id', 'delay_arrival', 'delay_departure', 'delay_at_station', 'day_of_week', 'is_weekend', 'is_peak_time', 'stop_lat', 'stop_lon']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ba084a2a-bb6a-4256-a292-e6f2713ae124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+--------------------+----------------+-------------------+------------------+----------------+-------------------+------------------+-------------+-----+\n",
      "|betriebstag|fahrt_bezeichner|betreiber_id|betreiber_abk|betreiber_name|produkt_id|linien_id|linien_text|umlauf_id|verkehrsmittel_text|zusatzfahrt_tf|faellt_aus_tf|  bpuic|   haltestellen_name|    ankunftszeit|        an_prognose|an_prognose_status|    abfahrtszeit|        ab_prognose|ab_prognose_status|durchfahrt_tf|month|\n",
      "+-----------+----------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+--------------------+----------------+-------------------+------------------+----------------+-------------------+------------------+-------------+-----+\n",
      "| 22.12.2023|  85:65:8031:001|       85:65|       THURBO|        THURBO|       Zug|     8031|        S30|         |                  S|         false|        false|8506101|   Felben-Wellhausen|22.12.2023 09:36|22.12.2023 09:36:34|              REAL|22.12.2023 09:36|22.12.2023 09:36:53|              REAL|        false|   12|\n",
      "| 22.12.2023|  85:65:8031:001|       85:65|       THURBO|        THURBO|       Zug|     8031|        S30|         |                  S|         false|        false|8506102|Hüttlingen-Metten...|22.12.2023 09:39|22.12.2023 09:39:08|              REAL|22.12.2023 09:39|22.12.2023 09:39:26|              REAL|        false|   12|\n",
      "| 22.12.2023|  85:65:8031:001|       85:65|       THURBO|        THURBO|       Zug|     8031|        S30|         |                  S|         false|        false|8506103|Müllheim-Wigoltingen|22.12.2023 09:42|22.12.2023 09:42:27|              REAL|22.12.2023 09:42|22.12.2023 09:43:48|              REAL|        false|   12|\n",
      "| 22.12.2023|  85:65:8031:001|       85:65|       THURBO|        THURBO|       Zug|     8031|        S30|         |                  S|         false|        false|8506104|          Märstetten|22.12.2023 09:45|22.12.2023 09:46:15|              REAL|22.12.2023 09:45|22.12.2023 09:46:40|              REAL|        false|   12|\n",
      "| 22.12.2023|  85:65:8031:001|       85:65|       THURBO|        THURBO|       Zug|     8031|        S30|         |                  S|         false|        false|8506105|          Weinfelden|22.12.2023 09:50|22.12.2023 09:50:40|              REAL|                |                   |                  |        false|   12|\n",
      "+-----------+----------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+--------------------+----------------+-------------------+------------------+----------------+-------------------+------------------+-------------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "spark.read.orc('/data/sbb/orc/istdaten/year=2023').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52877f-f6d9-4806-b6cd-f831ab1663c8",
   "metadata": {},
   "source": [
    "# Delay Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2527e021-a754-4c12-b8ac-45b5e5267871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+\n",
      "|               date|stop_id|transportation|             trip_id|delay_arrival|delay_departure|delay_at_station|day_of_week|is_weekend|is_peak_time|        stop_lat|        stop_lon|\n",
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+\n",
      "|2023-03-23 00:00:00|8501214|         Metro|85:151:TL070-4506...|           81|            155|              74|          3|     false|       false|46.5221956281987| 6.5661367555044|\n",
      "|2023-12-22 00:00:00|8506100|           Zug|      85:65:8032:001|           11|             18|               7|          4|     false|        true| 47.558162001326|8.89656423219735|\n",
      "|2023-12-22 00:00:00|8506101|           Zug|      85:65:8031:001|           34|             53|              19|          4|     false|       false|47.5771029633049| 8.9431778122968|\n",
      "|2023-12-22 00:00:00|8506101|           Zug|      85:65:8032:001|          -47|              2|              49|          4|     false|        true|47.5771029633049| 8.9431778122968|\n",
      "|2023-04-25 00:00:00|8507249|           BUS|85:839:165351-000...|           93|             96|               3|          1|     false|        true|47.1661422191549|8.51667568164316|\n",
      "|2023-04-25 00:00:00|8507249|           BUS|85:839:165454-000...|          130|            149|              19|          1|     false|       false|47.1661422191549|8.51667568164316|\n",
      "|2023-03-23 00:00:00|8530749|         Metro|85:151:TL070-4506...|          137|           null|            null|          3|     false|       false|46.5374792630591|6.57807536563201|\n",
      "|2023-03-18 00:00:00|8570850|           Bus|85:766:202055-621...|          210|           null|            null|          5|      true|       false|46.8756220371815|9.52330982286194|\n",
      "|2023-01-28 00:00:00|8573457|           Bus|85:836:293606-007...|           89|             90|               1|          5|      true|       false|47.6985075665959|8.62761761924736|\n",
      "|2023-10-16 00:00:00|8573457|           Bus|85:836:291284-007...|           42|             54|              12|          0|     false|       false|47.6985075665959|8.62761761924736|\n",
      "+-------------------+-------+--------------+--------------------+-------------+---------------+----------------+-----------+----------+------------+----------------+----------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "features = istdaten_df.select(['date','stop_id', 'transportation', 'trip_id', 'delay_arrival', 'delay_departure', 'delay_at_station', 'day_of_week', 'is_weekend', 'is_peak_time', 'stop_lat', 'stop_lon'])\n",
    "\n",
    "features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d52fa585-b96d-4bb5-93fe-e410048dc09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BUS', 'Tram', 'Zug', 'Bus', '', 'Zahnradbahn', 'Metro', 'CS', 'WM-BUS', 'Taxi']"
     ]
    }
   ],
   "source": [
    "distinct_transportation = istdaten_df.select(\"transportation\").distinct().collect()\n",
    "distinct_transportation = [row.transportation for row in distinct_transportation]\n",
    "print(distinct_transportation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "10da751c-1e4d-45b8-96cb-2d1a4524376c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(12,[0,7,10,11],[...|    2|\n",
      "|(12,[0,7,10,11],[...|   46|\n",
      "|(12,[0,7,9,10,11]...|  228|\n",
      "|(12,[0,7,10,11],[...|  -20|\n",
      "|(12,[0,7,9,10,11]...|   60|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "features = istdaten_df.select(['stop_id', 'transportation', 'delay_arrival', 'day_of_week', 'is_weekend', 'is_peak_time', 'stop_lat', 'stop_lon'])\\\n",
    "                        .withColumn(\"transportation\", lower(col(\"transportation\")))\\\n",
    "                        .withColumn(\"transportation\",\\\n",
    "                           when(col(\"transportation\")==\"\" ,None) \\\n",
    "                              .otherwise(col(\"transportation\")))\\\n",
    "                        .fillna({\n",
    "                            \"delay_arrival\": 0\n",
    "                        })\n",
    "\n",
    "transportation_indexer = StringIndexer(inputCol=\"transportation\", outputCol=\"transportation_index\")\n",
    "transportation_encoder = OneHotEncoder(inputCol=\"transportation_index\", outputCol=\"transportation_vec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"transportation_vec\",\n",
    "        \"day_of_week\",\n",
    "        \"is_weekend\",\n",
    "        \"is_peak_time\",\n",
    "        \"stop_lat\",\n",
    "        \"stop_lon\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    transportation_indexer,\n",
    "    transportation_encoder,\n",
    "    assembler\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline_model = pipeline.fit(features)\n",
    "prepared_features = pipeline_model.transform(features)\n",
    "\n",
    "# Select the features and the target variable for the model\n",
    "final_data = prepared_features.select(col(\"features\"), col(\"delay_arrival\").alias(\"label\"))\n",
    "\n",
    "final_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2542c198-e5bb-440c-ad92-12c58ec41f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o3957.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 767.0 failed 4 times, most recent failure: Lost task 0.3 in stage 767.0 (TID 45075) (iccluster072.iccluster.epfl.ch executor 2622): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$5584/0x00007f1dfd5559b0: (string) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1470)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2294)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:551)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:554)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n",
      "\t... 35 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2275)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2294)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1470)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1443)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$5584/0x00007f1dfd5559b0: (string) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1470)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2294)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:551)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:554)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n",
      "\t... 35 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 383, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 380, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o3957.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 767.0 failed 4 times, most recent failure: Lost task 0.3 in stage 767.0 (TID 45075) (iccluster072.iccluster.epfl.ch executor 2622): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$5584/0x00007f1dfd5559b0: (string) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1470)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2294)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:551)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:554)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n",
      "\t... 35 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2275)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2294)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1470)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1443)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$5584/0x00007f1dfd5559b0: (string) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1470)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2294)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:551)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:554)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n",
      "\t... 35 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxDepth=10)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n",
    "\n",
    "# Show some predictions\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b9958-70bc-4059-b1e1-53dd26b2408e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
